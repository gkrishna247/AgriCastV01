{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data\n",
    "def select_data(data, start=2016, end=2024, columns=None):\n",
    "    # convert the data to a pandas dataframe\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    # create a mask for the data\n",
    "    mask = (data['Date'].dt.year >= start) & (data['Date'].dt.year <= end)\n",
    "    # select the data\n",
    "    data = data.loc[mask]\n",
    "    if columns is not None:\n",
    "        data = data[columns]\n",
    "    else:\n",
    "        # drop the date column\n",
    "        data = data.drop(columns='Date')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), :]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, :])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaler(dataset):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    return scaler, dataset \n",
    "def to_scalar(scaler, data):\n",
    "    return scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dataset, look_back=1, epochs=100, batch_size=1, verbose=1):\n",
    "    X, Y = prepare_dataset(dataset, look_back)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2]))\n",
    "    Y = np.reshape(Y, (Y.shape[0], Y.shape[1]))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dense(X.shape[2]))  # Assuming single feature for simplicity\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data\n",
    "def select_data_01(data, start=2016, end=2024, columns=None):\n",
    "    # convert the data to a pandas dataframe\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    # create a mask for the data\n",
    "    mask = (data['Date'].dt.year >= start) & (data['Date'].dt.year <= end)\n",
    "    # select the data\n",
    "    data = data.loc[mask]\n",
    "    if columns is not None:\n",
    "        data = data[columns]\n",
    "    else:\n",
    "        # drop the date column\n",
    "        data = data.drop(columns='Date')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 5)\n"
     ]
    }
   ],
   "source": [
    "# declare the path of the data\n",
    "path_to_data = r\"data\\cleansing\\filled\\mean_filled.csv\"\n",
    "\n",
    "# load the data\n",
    "# import pandas as pd\n",
    "data = pd.read_csv(path_to_data)\n",
    "\n",
    "col=['Rice', 'Wheat', 'Atta (Wheat)', 'Gram Dal', 'Tur/Arhar Dal']\n",
    "\n",
    "# select the desired data\n",
    "req_data = select_data(data=data, start=2024, end=2024, columns=col)\n",
    "print(req_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in last\n",
    "col=['Date','Rice', 'Wheat', 'Atta (Wheat)', 'Gram Dal', 'Tur/Arhar Dal']\n",
    "inp_data = select_data_01(data=data, start=2024, end=2024, columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date   Rice  Wheat  Atta (Wheat)  Gram Dal  Tur/Arhar Dal\n",
      "2922 2024-01-01  59.04  44.45         54.08     86.88         171.79\n",
      "2923 2024-01-02  58.89  44.91         53.22     86.83         171.58\n",
      "2924 2024-01-03  58.74  45.12         52.94     86.80         170.77\n",
      "2925 2024-01-04  59.44  45.00         53.40     86.37         170.26\n",
      "2926 2024-01-05  60.03  44.69         53.00     86.88         170.38\n"
     ]
    }
   ],
   "source": [
    "print(inp_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "train_size = int(len(req_data) * 0.80)\n",
    "test_size = len(req_data) - train_size\n",
    "train, test = req_data[0:train_size], req_data[train_size:len(req_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler, train_scaler = create_scaler(train) # scaled_data is a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "myModel = create_model(train_scaler, look_back=1, epochs=100, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "# convert to scaler\n",
    "test_scalar = scaler.transform(test)\n",
    "# split features and outcome\n",
    "x_test, y_test = prepare_dataset(test_scalar, look_back=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = prepare_dataset(train_scaler, look_back=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "# predict the results\n",
    "test_pred_scalar = myModel.predict(x_test)\n",
    "train_pred_scalar = myModel.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the predictions\n",
    "test_pred = scaler.inverse_transform(test_pred_scalar)\n",
    "train_pred = scaler.inverse_transform(train_pred_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 83.414795629143\n",
      "Test Error: 86.79806511415654\n"
     ]
    }
   ],
   "source": [
    "# compare the predictions with the actual data\n",
    "# train_pred with y_train\n",
    "# test_pred with y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 5)\n",
      "(198, 5)\n",
      "(49, 5)\n",
      "(49, 5)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the data\n",
    "print(train_pred.shape)\n",
    "print(y_train.shape)\n",
    "print(test_pred.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_data = create_input_data(myModel, scaler, inp_data, date='2024-01-01', look_back=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(new_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_data(model, scaler, dataset, date, look_back=1):\n",
    "    # date format: 'yyyy-mm-dd'\n",
    "    #scaled_data = scaler.transform(dataset)\n",
    "\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
    "\n",
    "    # find the start and end date of dataset\n",
    "    start_date = dataset['Date'].min()\n",
    "    end_date = dataset['Date'].max()\n",
    "\n",
    "    # chect if the date is in the dataset\n",
    "\n",
    "    # chect if the date is within the range of the dataset\n",
    "    if pd.to_datetime(date) <= end_date:\n",
    "        # take the last look_back days\n",
    "        to_date = pd.to_datetime(date)\n",
    "        from_date = to_date - pd.DateOffset(days=look_back-1)\n",
    "        mask = (dataset['Date'] >= from_date) & (dataset['Date'] <= to_date)\n",
    "        input_data = dataset.loc[mask]\n",
    "        input_data = input_data.drop(columns='Date')\n",
    "        input_data = scaler.transform(input_data)\n",
    "        X = []\n",
    "        for i in range(len(input_data) - look_back):\n",
    "            a = input_data[i:(i + look_back), :]\n",
    "            X.append(a)\n",
    "        X = np.array(X)\n",
    "        return X\n",
    "    else:\n",
    "        # take the last look_back days\n",
    "        req_to_date = pd.to_datetime(date)\n",
    "        #req_from_date = req_to_date - pd.DateOffset(days=look_back-1)\n",
    "        to_date = end_date\n",
    "        from_date = to_date - pd.DateOffset(days=look_back-1)\n",
    "        mask = (dataset['Date'] >= from_date) & (dataset['Date'] <= to_date)\n",
    "        input_data = dataset.loc[mask]\n",
    "        input_data = input_data.drop(columns='Date')\n",
    "        input_data = scaler.transform(input_data)\n",
    "        # calculate the number of days to predict\n",
    "        days_to_predict = (req_to_date - to_date).days\n",
    "        for i in range(days_to_predict):\n",
    "            X = []\n",
    "            for i in range(len(input_data) - look_back):\n",
    "                a = input_data[i:(i + look_back), :]\n",
    "                X.append(a)\n",
    "            X = np.array(X)\n",
    "            y_pred = model.predict(X)\n",
    "            input_data = np.append(input_data, y_pred, axis=0)\n",
    "            input_data = input_data[1:]\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    X = []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back), :]\n",
    "        X.append(a)\n",
    "    return np.array(X) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(path_to_data)\n",
    "print(d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
