{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5ncTnK8IGcE2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"data columns are: Index(['Day', 'Month', 'Year', 'States/UTs', 'Rice', 'Wheat', 'Atta (Wheat)',\\n       'Gram Dal', 'Tur/Arhar Dal', 'Urad Dal', 'Moong Dal', 'Masoor Dal',\\n       'Sugar', 'Milk @', 'Groundnut Oil (Packed)', 'Mustard Oil (Packed)',\\n       'Vanaspati (Packed)', 'Soya Oil (Packed)', 'Sunflower Oil (Packed)',\\n       'Palm Oil (Packed)', 'Gur', 'Tea Loose', 'Salt Pack (Iodised)',\\n       'Potato', 'Onion', 'Tomato'],\\n      dtype='object')\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''data columns are: Index(['Day', 'Month', 'Year', 'States/UTs', 'Rice', 'Wheat', 'Atta (Wheat)',\n",
        "       'Gram Dal', 'Tur/Arhar Dal', 'Urad Dal', 'Moong Dal', 'Masoor Dal',\n",
        "       'Sugar', 'Milk @', 'Groundnut Oil (Packed)', 'Mustard Oil (Packed)',\n",
        "       'Vanaspati (Packed)', 'Soya Oil (Packed)', 'Sunflower Oil (Packed)',\n",
        "       'Palm Oil (Packed)', 'Gur', 'Tea Loose', 'Salt Pack (Iodised)',\n",
        "       'Potato', 'Onion', 'Tomato'],\n",
        "      dtype='object')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jKzOr6xKG4Ys"
      },
      "outputs": [],
      "source": [
        "# Load the data from the CSV file\n",
        "df = pd.read_csv(r\"data\\cleansing\\filled\\newdata.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>Rice</th>\n",
              "      <th>Wheat</th>\n",
              "      <th>Atta (Wheat)</th>\n",
              "      <th>Gram Dal</th>\n",
              "      <th>Tur/Arhar Dal</th>\n",
              "      <th>Urad Dal</th>\n",
              "      <th>Moong Dal</th>\n",
              "      <th>...</th>\n",
              "      <th>Vanaspati (Packed)</th>\n",
              "      <th>Soya Oil (Packed)</th>\n",
              "      <th>Sunflower Oil (Packed)</th>\n",
              "      <th>Palm Oil (Packed)</th>\n",
              "      <th>Gur</th>\n",
              "      <th>Tea Loose</th>\n",
              "      <th>Salt Pack (Iodised)</th>\n",
              "      <th>Potato</th>\n",
              "      <th>Onion</th>\n",
              "      <th>Tomato</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.50</td>\n",
              "      <td>30.50</td>\n",
              "      <td>26.67</td>\n",
              "      <td>70.50</td>\n",
              "      <td>155.00</td>\n",
              "      <td>160.00</td>\n",
              "      <td>105.75</td>\n",
              "      <td>...</td>\n",
              "      <td>79.25</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.25</td>\n",
              "      <td>59.25</td>\n",
              "      <td>43.00</td>\n",
              "      <td>215.00</td>\n",
              "      <td>16.25</td>\n",
              "      <td>20.50</td>\n",
              "      <td>23.25</td>\n",
              "      <td>29.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.60</td>\n",
              "      <td>31.40</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.40</td>\n",
              "      <td>154.60</td>\n",
              "      <td>161.20</td>\n",
              "      <td>108.20</td>\n",
              "      <td>...</td>\n",
              "      <td>82.00</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.00</td>\n",
              "      <td>59.00</td>\n",
              "      <td>45.00</td>\n",
              "      <td>216.00</td>\n",
              "      <td>16.60</td>\n",
              "      <td>20.40</td>\n",
              "      <td>23.60</td>\n",
              "      <td>32.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.60</td>\n",
              "      <td>31.40</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.40</td>\n",
              "      <td>154.00</td>\n",
              "      <td>161.20</td>\n",
              "      <td>108.20</td>\n",
              "      <td>...</td>\n",
              "      <td>82.00</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.00</td>\n",
              "      <td>59.00</td>\n",
              "      <td>45.00</td>\n",
              "      <td>216.00</td>\n",
              "      <td>16.60</td>\n",
              "      <td>20.40</td>\n",
              "      <td>23.60</td>\n",
              "      <td>35.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.60</td>\n",
              "      <td>31.40</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.40</td>\n",
              "      <td>151.60</td>\n",
              "      <td>160.20</td>\n",
              "      <td>107.20</td>\n",
              "      <td>...</td>\n",
              "      <td>82.00</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.00</td>\n",
              "      <td>59.00</td>\n",
              "      <td>45.00</td>\n",
              "      <td>216.00</td>\n",
              "      <td>16.60</td>\n",
              "      <td>21.40</td>\n",
              "      <td>23.00</td>\n",
              "      <td>41.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.50</td>\n",
              "      <td>31.75</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.75</td>\n",
              "      <td>151.50</td>\n",
              "      <td>161.50</td>\n",
              "      <td>107.50</td>\n",
              "      <td>...</td>\n",
              "      <td>80.25</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>91.75</td>\n",
              "      <td>58.75</td>\n",
              "      <td>45.75</td>\n",
              "      <td>215.00</td>\n",
              "      <td>17.00</td>\n",
              "      <td>21.25</td>\n",
              "      <td>22.75</td>\n",
              "      <td>41.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3141</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>56.12</td>\n",
              "      <td>44.88</td>\n",
              "      <td>52.88</td>\n",
              "      <td>99.76</td>\n",
              "      <td>172.82</td>\n",
              "      <td>137.65</td>\n",
              "      <td>120.76</td>\n",
              "      <td>...</td>\n",
              "      <td>126.29</td>\n",
              "      <td>120.706538</td>\n",
              "      <td>115.82</td>\n",
              "      <td>95.59</td>\n",
              "      <td>57.53</td>\n",
              "      <td>286.23</td>\n",
              "      <td>24.41</td>\n",
              "      <td>49.00</td>\n",
              "      <td>53.47</td>\n",
              "      <td>23.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3142</th>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.31</td>\n",
              "      <td>45.73</td>\n",
              "      <td>54.06</td>\n",
              "      <td>99.75</td>\n",
              "      <td>173.53</td>\n",
              "      <td>139.00</td>\n",
              "      <td>122.47</td>\n",
              "      <td>...</td>\n",
              "      <td>126.69</td>\n",
              "      <td>125.605161</td>\n",
              "      <td>116.00</td>\n",
              "      <td>95.78</td>\n",
              "      <td>57.19</td>\n",
              "      <td>283.88</td>\n",
              "      <td>24.16</td>\n",
              "      <td>48.97</td>\n",
              "      <td>53.84</td>\n",
              "      <td>23.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3143</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.53</td>\n",
              "      <td>45.80</td>\n",
              "      <td>54.75</td>\n",
              "      <td>99.88</td>\n",
              "      <td>172.23</td>\n",
              "      <td>138.19</td>\n",
              "      <td>121.97</td>\n",
              "      <td>...</td>\n",
              "      <td>126.38</td>\n",
              "      <td>125.397419</td>\n",
              "      <td>116.06</td>\n",
              "      <td>95.91</td>\n",
              "      <td>57.34</td>\n",
              "      <td>283.96</td>\n",
              "      <td>24.19</td>\n",
              "      <td>48.88</td>\n",
              "      <td>54.41</td>\n",
              "      <td>24.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3144</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.29</td>\n",
              "      <td>45.79</td>\n",
              "      <td>55.19</td>\n",
              "      <td>99.48</td>\n",
              "      <td>172.30</td>\n",
              "      <td>138.90</td>\n",
              "      <td>122.03</td>\n",
              "      <td>...</td>\n",
              "      <td>127.16</td>\n",
              "      <td>125.459032</td>\n",
              "      <td>116.16</td>\n",
              "      <td>95.97</td>\n",
              "      <td>56.84</td>\n",
              "      <td>287.13</td>\n",
              "      <td>24.07</td>\n",
              "      <td>49.32</td>\n",
              "      <td>55.45</td>\n",
              "      <td>26.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3145</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.47</td>\n",
              "      <td>45.64</td>\n",
              "      <td>53.77</td>\n",
              "      <td>100.30</td>\n",
              "      <td>173.54</td>\n",
              "      <td>139.57</td>\n",
              "      <td>122.27</td>\n",
              "      <td>...</td>\n",
              "      <td>127.37</td>\n",
              "      <td>125.307419</td>\n",
              "      <td>116.40</td>\n",
              "      <td>96.73</td>\n",
              "      <td>57.53</td>\n",
              "      <td>284.17</td>\n",
              "      <td>24.55</td>\n",
              "      <td>49.90</td>\n",
              "      <td>56.00</td>\n",
              "      <td>27.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3146 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Day  Month  Year   Rice  Wheat  Atta (Wheat)  Gram Dal  Tur/Arhar Dal  \\\n",
              "0       1      1  2016  29.50  30.50         26.67     70.50         155.00   \n",
              "1       2      1  2016  29.60  31.40         27.50     71.40         154.60   \n",
              "2       3      1  2016  29.60  31.40         27.50     71.40         154.00   \n",
              "3       4      1  2016  29.60  31.40         27.50     71.40         151.60   \n",
              "4       5      1  2016  29.50  31.75         27.50     71.75         151.50   \n",
              "...   ...    ...   ...    ...    ...           ...       ...            ...   \n",
              "3141    1      9  2024  56.12  44.88         52.88     99.76         172.82   \n",
              "3142    2      9  2024  57.31  45.73         54.06     99.75         173.53   \n",
              "3143    3      9  2024  57.53  45.80         54.75     99.88         172.23   \n",
              "3144    4      9  2024  57.29  45.79         55.19     99.48         172.30   \n",
              "3145    5      9  2024  57.47  45.64         53.77    100.30         173.54   \n",
              "\n",
              "      Urad Dal  Moong Dal  ...  Vanaspati (Packed)  Soya Oil (Packed)  \\\n",
              "0       160.00     105.75  ...               79.25          88.000000   \n",
              "1       161.20     108.20  ...               82.00          88.000000   \n",
              "2       161.20     108.20  ...               82.00          88.000000   \n",
              "3       160.20     107.20  ...               82.00          88.000000   \n",
              "4       161.50     107.50  ...               80.25          88.000000   \n",
              "...        ...        ...  ...                 ...                ...   \n",
              "3141    137.65     120.76  ...              126.29         120.706538   \n",
              "3142    139.00     122.47  ...              126.69         125.605161   \n",
              "3143    138.19     121.97  ...              126.38         125.397419   \n",
              "3144    138.90     122.03  ...              127.16         125.459032   \n",
              "3145    139.57     122.27  ...              127.37         125.307419   \n",
              "\n",
              "      Sunflower Oil (Packed)  Palm Oil (Packed)    Gur  Tea Loose  \\\n",
              "0                      92.25              59.25  43.00     215.00   \n",
              "1                      92.00              59.00  45.00     216.00   \n",
              "2                      92.00              59.00  45.00     216.00   \n",
              "3                      92.00              59.00  45.00     216.00   \n",
              "4                      91.75              58.75  45.75     215.00   \n",
              "...                      ...                ...    ...        ...   \n",
              "3141                  115.82              95.59  57.53     286.23   \n",
              "3142                  116.00              95.78  57.19     283.88   \n",
              "3143                  116.06              95.91  57.34     283.96   \n",
              "3144                  116.16              95.97  56.84     287.13   \n",
              "3145                  116.40              96.73  57.53     284.17   \n",
              "\n",
              "      Salt Pack (Iodised)  Potato  Onion  Tomato  \n",
              "0                   16.25   20.50  23.25   29.50  \n",
              "1                   16.60   20.40  23.60   32.00  \n",
              "2                   16.60   20.40  23.60   35.00  \n",
              "3                   16.60   21.40  23.00   41.40  \n",
              "4                   17.00   21.25  22.75   41.00  \n",
              "...                   ...     ...    ...     ...  \n",
              "3141                24.41   49.00  53.47   23.76  \n",
              "3142                24.16   48.97  53.84   23.94  \n",
              "3143                24.19   48.88  54.41   24.53  \n",
              "3144                24.07   49.32  55.45   26.58  \n",
              "3145                24.55   49.90  56.00   27.13  \n",
              "\n",
              "[3146 rows x 25 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3146, 25)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df[df['Year']>2022]\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TmhegmtXG4QW"
      },
      "outputs": [],
      "source": [
        "# Convert date columns to a datetime object\n",
        "df['Date'] = pd.to_datetime(df[['Day', 'Month', 'Year']])\n",
        "df.set_index('Date', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>Rice</th>\n",
              "      <th>Wheat</th>\n",
              "      <th>Atta (Wheat)</th>\n",
              "      <th>Gram Dal</th>\n",
              "      <th>Tur/Arhar Dal</th>\n",
              "      <th>Urad Dal</th>\n",
              "      <th>Moong Dal</th>\n",
              "      <th>...</th>\n",
              "      <th>Vanaspati (Packed)</th>\n",
              "      <th>Soya Oil (Packed)</th>\n",
              "      <th>Sunflower Oil (Packed)</th>\n",
              "      <th>Palm Oil (Packed)</th>\n",
              "      <th>Gur</th>\n",
              "      <th>Tea Loose</th>\n",
              "      <th>Salt Pack (Iodised)</th>\n",
              "      <th>Potato</th>\n",
              "      <th>Onion</th>\n",
              "      <th>Tomato</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-01</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.50</td>\n",
              "      <td>30.50</td>\n",
              "      <td>26.67</td>\n",
              "      <td>70.50</td>\n",
              "      <td>155.00</td>\n",
              "      <td>160.00</td>\n",
              "      <td>105.75</td>\n",
              "      <td>...</td>\n",
              "      <td>79.25</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.25</td>\n",
              "      <td>59.25</td>\n",
              "      <td>43.00</td>\n",
              "      <td>215.00</td>\n",
              "      <td>16.25</td>\n",
              "      <td>20.50</td>\n",
              "      <td>23.25</td>\n",
              "      <td>29.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-02</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.60</td>\n",
              "      <td>31.40</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.40</td>\n",
              "      <td>154.60</td>\n",
              "      <td>161.20</td>\n",
              "      <td>108.20</td>\n",
              "      <td>...</td>\n",
              "      <td>82.00</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.00</td>\n",
              "      <td>59.00</td>\n",
              "      <td>45.00</td>\n",
              "      <td>216.00</td>\n",
              "      <td>16.60</td>\n",
              "      <td>20.40</td>\n",
              "      <td>23.60</td>\n",
              "      <td>32.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-03</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.60</td>\n",
              "      <td>31.40</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.40</td>\n",
              "      <td>154.00</td>\n",
              "      <td>161.20</td>\n",
              "      <td>108.20</td>\n",
              "      <td>...</td>\n",
              "      <td>82.00</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.00</td>\n",
              "      <td>59.00</td>\n",
              "      <td>45.00</td>\n",
              "      <td>216.00</td>\n",
              "      <td>16.60</td>\n",
              "      <td>20.40</td>\n",
              "      <td>23.60</td>\n",
              "      <td>35.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-04</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.60</td>\n",
              "      <td>31.40</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.40</td>\n",
              "      <td>151.60</td>\n",
              "      <td>160.20</td>\n",
              "      <td>107.20</td>\n",
              "      <td>...</td>\n",
              "      <td>82.00</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>92.00</td>\n",
              "      <td>59.00</td>\n",
              "      <td>45.00</td>\n",
              "      <td>216.00</td>\n",
              "      <td>16.60</td>\n",
              "      <td>21.40</td>\n",
              "      <td>23.00</td>\n",
              "      <td>41.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-05</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>29.50</td>\n",
              "      <td>31.75</td>\n",
              "      <td>27.50</td>\n",
              "      <td>71.75</td>\n",
              "      <td>151.50</td>\n",
              "      <td>161.50</td>\n",
              "      <td>107.50</td>\n",
              "      <td>...</td>\n",
              "      <td>80.25</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>91.75</td>\n",
              "      <td>58.75</td>\n",
              "      <td>45.75</td>\n",
              "      <td>215.00</td>\n",
              "      <td>17.00</td>\n",
              "      <td>21.25</td>\n",
              "      <td>22.75</td>\n",
              "      <td>41.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-09-01</th>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>56.12</td>\n",
              "      <td>44.88</td>\n",
              "      <td>52.88</td>\n",
              "      <td>99.76</td>\n",
              "      <td>172.82</td>\n",
              "      <td>137.65</td>\n",
              "      <td>120.76</td>\n",
              "      <td>...</td>\n",
              "      <td>126.29</td>\n",
              "      <td>120.706538</td>\n",
              "      <td>115.82</td>\n",
              "      <td>95.59</td>\n",
              "      <td>57.53</td>\n",
              "      <td>286.23</td>\n",
              "      <td>24.41</td>\n",
              "      <td>49.00</td>\n",
              "      <td>53.47</td>\n",
              "      <td>23.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-09-02</th>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.31</td>\n",
              "      <td>45.73</td>\n",
              "      <td>54.06</td>\n",
              "      <td>99.75</td>\n",
              "      <td>173.53</td>\n",
              "      <td>139.00</td>\n",
              "      <td>122.47</td>\n",
              "      <td>...</td>\n",
              "      <td>126.69</td>\n",
              "      <td>125.605161</td>\n",
              "      <td>116.00</td>\n",
              "      <td>95.78</td>\n",
              "      <td>57.19</td>\n",
              "      <td>283.88</td>\n",
              "      <td>24.16</td>\n",
              "      <td>48.97</td>\n",
              "      <td>53.84</td>\n",
              "      <td>23.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-09-03</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.53</td>\n",
              "      <td>45.80</td>\n",
              "      <td>54.75</td>\n",
              "      <td>99.88</td>\n",
              "      <td>172.23</td>\n",
              "      <td>138.19</td>\n",
              "      <td>121.97</td>\n",
              "      <td>...</td>\n",
              "      <td>126.38</td>\n",
              "      <td>125.397419</td>\n",
              "      <td>116.06</td>\n",
              "      <td>95.91</td>\n",
              "      <td>57.34</td>\n",
              "      <td>283.96</td>\n",
              "      <td>24.19</td>\n",
              "      <td>48.88</td>\n",
              "      <td>54.41</td>\n",
              "      <td>24.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-09-04</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.29</td>\n",
              "      <td>45.79</td>\n",
              "      <td>55.19</td>\n",
              "      <td>99.48</td>\n",
              "      <td>172.30</td>\n",
              "      <td>138.90</td>\n",
              "      <td>122.03</td>\n",
              "      <td>...</td>\n",
              "      <td>127.16</td>\n",
              "      <td>125.459032</td>\n",
              "      <td>116.16</td>\n",
              "      <td>95.97</td>\n",
              "      <td>56.84</td>\n",
              "      <td>287.13</td>\n",
              "      <td>24.07</td>\n",
              "      <td>49.32</td>\n",
              "      <td>55.45</td>\n",
              "      <td>26.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-09-05</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>2024</td>\n",
              "      <td>57.47</td>\n",
              "      <td>45.64</td>\n",
              "      <td>53.77</td>\n",
              "      <td>100.30</td>\n",
              "      <td>173.54</td>\n",
              "      <td>139.57</td>\n",
              "      <td>122.27</td>\n",
              "      <td>...</td>\n",
              "      <td>127.37</td>\n",
              "      <td>125.307419</td>\n",
              "      <td>116.40</td>\n",
              "      <td>96.73</td>\n",
              "      <td>57.53</td>\n",
              "      <td>284.17</td>\n",
              "      <td>24.55</td>\n",
              "      <td>49.90</td>\n",
              "      <td>56.00</td>\n",
              "      <td>27.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3146 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Day  Month  Year   Rice  Wheat  Atta (Wheat)  Gram Dal  \\\n",
              "Date                                                                 \n",
              "2016-01-01    1      1  2016  29.50  30.50         26.67     70.50   \n",
              "2016-01-02    2      1  2016  29.60  31.40         27.50     71.40   \n",
              "2016-01-03    3      1  2016  29.60  31.40         27.50     71.40   \n",
              "2016-01-04    4      1  2016  29.60  31.40         27.50     71.40   \n",
              "2016-01-05    5      1  2016  29.50  31.75         27.50     71.75   \n",
              "...         ...    ...   ...    ...    ...           ...       ...   \n",
              "2024-09-01    1      9  2024  56.12  44.88         52.88     99.76   \n",
              "2024-09-02    2      9  2024  57.31  45.73         54.06     99.75   \n",
              "2024-09-03    3      9  2024  57.53  45.80         54.75     99.88   \n",
              "2024-09-04    4      9  2024  57.29  45.79         55.19     99.48   \n",
              "2024-09-05    5      9  2024  57.47  45.64         53.77    100.30   \n",
              "\n",
              "            Tur/Arhar Dal  Urad Dal  Moong Dal  ...  Vanaspati (Packed)  \\\n",
              "Date                                            ...                       \n",
              "2016-01-01         155.00    160.00     105.75  ...               79.25   \n",
              "2016-01-02         154.60    161.20     108.20  ...               82.00   \n",
              "2016-01-03         154.00    161.20     108.20  ...               82.00   \n",
              "2016-01-04         151.60    160.20     107.20  ...               82.00   \n",
              "2016-01-05         151.50    161.50     107.50  ...               80.25   \n",
              "...                   ...       ...        ...  ...                 ...   \n",
              "2024-09-01         172.82    137.65     120.76  ...              126.29   \n",
              "2024-09-02         173.53    139.00     122.47  ...              126.69   \n",
              "2024-09-03         172.23    138.19     121.97  ...              126.38   \n",
              "2024-09-04         172.30    138.90     122.03  ...              127.16   \n",
              "2024-09-05         173.54    139.57     122.27  ...              127.37   \n",
              "\n",
              "            Soya Oil (Packed)  Sunflower Oil (Packed)  Palm Oil (Packed)  \\\n",
              "Date                                                                       \n",
              "2016-01-01          88.000000                   92.25              59.25   \n",
              "2016-01-02          88.000000                   92.00              59.00   \n",
              "2016-01-03          88.000000                   92.00              59.00   \n",
              "2016-01-04          88.000000                   92.00              59.00   \n",
              "2016-01-05          88.000000                   91.75              58.75   \n",
              "...                       ...                     ...                ...   \n",
              "2024-09-01         120.706538                  115.82              95.59   \n",
              "2024-09-02         125.605161                  116.00              95.78   \n",
              "2024-09-03         125.397419                  116.06              95.91   \n",
              "2024-09-04         125.459032                  116.16              95.97   \n",
              "2024-09-05         125.307419                  116.40              96.73   \n",
              "\n",
              "              Gur  Tea Loose  Salt Pack (Iodised)  Potato  Onion  Tomato  \n",
              "Date                                                                      \n",
              "2016-01-01  43.00     215.00                16.25   20.50  23.25   29.50  \n",
              "2016-01-02  45.00     216.00                16.60   20.40  23.60   32.00  \n",
              "2016-01-03  45.00     216.00                16.60   20.40  23.60   35.00  \n",
              "2016-01-04  45.00     216.00                16.60   21.40  23.00   41.40  \n",
              "2016-01-05  45.75     215.00                17.00   21.25  22.75   41.00  \n",
              "...           ...        ...                  ...     ...    ...     ...  \n",
              "2024-09-01  57.53     286.23                24.41   49.00  53.47   23.76  \n",
              "2024-09-02  57.19     283.88                24.16   48.97  53.84   23.94  \n",
              "2024-09-03  57.34     283.96                24.19   48.88  54.41   24.53  \n",
              "2024-09-04  56.84     287.13                24.07   49.32  55.45   26.58  \n",
              "2024-09-05  57.53     284.17                24.55   49.90  56.00   27.13  \n",
              "\n",
              "[3146 rows x 25 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pqLE05AUG4Jw"
      },
      "outputs": [],
      "source": [
        "# Select the crops for prediction\n",
        "crops = ['Rice', 'Wheat', 'Atta (Wheat)',\n",
        "       'Gram Dal', 'Tur/Arhar Dal', 'Urad Dal', 'Moong Dal', 'Masoor Dal',\n",
        "       'Sugar', 'Milk @', 'Groundnut Oil (Packed)', 'Mustard Oil (Packed)',\n",
        "       'Vanaspati (Packed)', 'Soya Oil (Packed)', 'Sunflower Oil (Packed)',\n",
        "       'Palm Oil (Packed)', 'Gur', 'Tea Loose', 'Salt Pack (Iodised)',\n",
        "       'Potato', 'Onion', 'Tomato']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "print(len(crops))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jb6B6siNG4Fo"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to store the last known data for each state\n",
        "last_known_data = {}\n",
        "last_known_data[\"tn\"] = df.iloc[-1][crops].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            Day  Month  Year   Rice  Wheat  Atta (Wheat)  Gram Dal  \\\n",
            "Date                                                                 \n",
            "2016-01-01    1      1  2016  29.50  30.50         26.67     70.50   \n",
            "2016-01-02    2      1  2016  29.60  31.40         27.50     71.40   \n",
            "2016-01-03    3      1  2016  29.60  31.40         27.50     71.40   \n",
            "2016-01-04    4      1  2016  29.60  31.40         27.50     71.40   \n",
            "2016-01-05    5      1  2016  29.50  31.75         27.50     71.75   \n",
            "...         ...    ...   ...    ...    ...           ...       ...   \n",
            "2024-09-01    1      9  2024  56.12  44.88         52.88     99.76   \n",
            "2024-09-02    2      9  2024  57.31  45.73         54.06     99.75   \n",
            "2024-09-03    3      9  2024  57.53  45.80         54.75     99.88   \n",
            "2024-09-04    4      9  2024  57.29  45.79         55.19     99.48   \n",
            "2024-09-05    5      9  2024  57.47  45.64         53.77    100.30   \n",
            "\n",
            "            Tur/Arhar Dal  Urad Dal  Moong Dal  ...  Vanaspati (Packed)  \\\n",
            "Date                                            ...                       \n",
            "2016-01-01         155.00    160.00     105.75  ...               79.25   \n",
            "2016-01-02         154.60    161.20     108.20  ...               82.00   \n",
            "2016-01-03         154.00    161.20     108.20  ...               82.00   \n",
            "2016-01-04         151.60    160.20     107.20  ...               82.00   \n",
            "2016-01-05         151.50    161.50     107.50  ...               80.25   \n",
            "...                   ...       ...        ...  ...                 ...   \n",
            "2024-09-01         172.82    137.65     120.76  ...              126.29   \n",
            "2024-09-02         173.53    139.00     122.47  ...              126.69   \n",
            "2024-09-03         172.23    138.19     121.97  ...              126.38   \n",
            "2024-09-04         172.30    138.90     122.03  ...              127.16   \n",
            "2024-09-05         173.54    139.57     122.27  ...              127.37   \n",
            "\n",
            "            Soya Oil (Packed)  Sunflower Oil (Packed)  Palm Oil (Packed)  \\\n",
            "Date                                                                       \n",
            "2016-01-01          88.000000                   92.25              59.25   \n",
            "2016-01-02          88.000000                   92.00              59.00   \n",
            "2016-01-03          88.000000                   92.00              59.00   \n",
            "2016-01-04          88.000000                   92.00              59.00   \n",
            "2016-01-05          88.000000                   91.75              58.75   \n",
            "...                       ...                     ...                ...   \n",
            "2024-09-01         120.706538                  115.82              95.59   \n",
            "2024-09-02         125.605161                  116.00              95.78   \n",
            "2024-09-03         125.397419                  116.06              95.91   \n",
            "2024-09-04         125.459032                  116.16              95.97   \n",
            "2024-09-05         125.307419                  116.40              96.73   \n",
            "\n",
            "              Gur  Tea Loose  Salt Pack (Iodised)  Potato  Onion  Tomato  \n",
            "Date                                                                      \n",
            "2016-01-01  43.00     215.00                16.25   20.50  23.25   29.50  \n",
            "2016-01-02  45.00     216.00                16.60   20.40  23.60   32.00  \n",
            "2016-01-03  45.00     216.00                16.60   20.40  23.60   35.00  \n",
            "2016-01-04  45.00     216.00                16.60   21.40  23.00   41.40  \n",
            "2016-01-05  45.75     215.00                17.00   21.25  22.75   41.00  \n",
            "...           ...        ...                  ...     ...    ...     ...  \n",
            "2024-09-01  57.53     286.23                24.41   49.00  53.47   23.76  \n",
            "2024-09-02  57.19     283.88                24.16   48.97  53.84   23.94  \n",
            "2024-09-03  57.34     283.96                24.19   48.88  54.41   24.53  \n",
            "2024-09-04  56.84     287.13                24.07   49.32  55.45   26.58  \n",
            "2024-09-05  57.53     284.17                24.55   49.90  56.00   27.13  \n",
            "\n",
            "[3146 rows x 25 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K17VZZ5tG4Bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3146, 22)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[29.5 , 30.5 , 26.67, ..., 20.5 , 23.25, 29.5 ],\n",
              "       [29.6 , 31.4 , 27.5 , ..., 20.4 , 23.6 , 32.  ],\n",
              "       [29.6 , 31.4 , 27.5 , ..., 20.4 , 23.6 , 35.  ],\n",
              "       ...,\n",
              "       [57.53, 45.8 , 54.75, ..., 48.88, 54.41, 24.53],\n",
              "       [57.29, 45.79, 55.19, ..., 49.32, 55.45, 26.58],\n",
              "       [57.47, 45.64, 53.77, ..., 49.9 , 56.  , 27.13]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prepare data for LSTM\n",
        "data = df[crops].values\n",
        "print(data.shape)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yqTQqxoVG39I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.062744   0.08675535 0.         ... 0.1292732  0.08383234 0.1611815 ]\n",
            " [0.06553263 0.13880856 0.02547575 ... 0.12640046 0.08662675 0.17992353]\n",
            " [0.06553263 0.13880856 0.02547575 ... 0.12640046 0.08662675 0.20241397]\n",
            " ...\n",
            " [0.84439487 0.97165992 0.86187845 ... 0.94455616 0.33261477 0.12392233]\n",
            " [0.83770218 0.97108155 0.87538367 ... 0.95719621 0.34091816 0.1392908 ]\n",
            " [0.8427217  0.96240602 0.83179865 ... 0.97385809 0.34530938 0.14341405]]\n"
          ]
        }
      ],
      "source": [
        "# Scale the data to be between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OMsCIA9OG31R"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.062744  , 0.08675535, 0.        , ..., 0.1292732 , 0.08383234,\n",
              "        0.1611815 ],\n",
              "       [0.06553263, 0.13880856, 0.02547575, ..., 0.12640046, 0.08662675,\n",
              "        0.17992353],\n",
              "       [0.06553263, 0.13880856, 0.02547575, ..., 0.12640046, 0.08662675,\n",
              "        0.20241397],\n",
              "       ...,\n",
              "       [0.66229782, 0.64256796, 0.68201351, ..., 0.74059178, 0.17053892,\n",
              "        0.11327686],\n",
              "       [0.66229782, 0.64256796, 0.68201351, ..., 0.76299914, 0.16878244,\n",
              "        0.13164405],\n",
              "       [0.66229782, 0.64256796, 0.68201351, ..., 0.74691181, 0.16702595,\n",
              "        0.1357673 ]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(scaled_data, test_size=0.2,shuffle=False)\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape:  (2516, 22)\n",
            "Test data shape:  (630, 22)\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data shape: \", train_data.shape)\n",
        "print(\"Test data shape: \", test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O4Eh9I7LG3xA"
      },
      "outputs": [],
      "source": [
        "# Create the function to create the dataset\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), :]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + look_back, :])\n",
        "    return np.array(X), np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WgrgJmkuG3s0"
      },
      "outputs": [],
      "source": [
        "# Set look_back period (number of previous days to consider)\n",
        "look_back = 7\n",
        "X_train, Y_train = create_dataset(train_data, look_back)\n",
        "X_test, Y_test = create_dataset(test_data, look_back)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[0.062744  , 0.08675535, 0.        , ..., 0.1292732 ,\n",
              "         0.08383234, 0.1611815 ],\n",
              "        [0.06553263, 0.13880856, 0.02547575, ..., 0.12640046,\n",
              "         0.08662675, 0.17992353],\n",
              "        [0.06553263, 0.13880856, 0.02547575, ..., 0.12640046,\n",
              "         0.08662675, 0.20241397],\n",
              "        ...,\n",
              "        [0.062744  , 0.15905147, 0.02547575, ..., 0.15081873,\n",
              "         0.07984032, 0.24739486],\n",
              "        [0.05995538, 0.12724118, 0.02547575, ..., 0.14363689,\n",
              "         0.08822355, 0.27438339],\n",
              "        [0.05995538, 0.12724118, 0.02547575, ..., 0.16087331,\n",
              "         0.08343313, 0.28337956]],\n",
              "\n",
              "       [[0.06553263, 0.13880856, 0.02547575, ..., 0.12640046,\n",
              "         0.08662675, 0.17992353],\n",
              "        [0.06553263, 0.13880856, 0.02547575, ..., 0.12640046,\n",
              "         0.08662675, 0.20241397],\n",
              "        [0.06553263, 0.13880856, 0.02547575, ..., 0.15512784,\n",
              "         0.08183633, 0.25039358],\n",
              "        ...,\n",
              "        [0.05995538, 0.12724118, 0.02547575, ..., 0.14363689,\n",
              "         0.08822355, 0.27438339],\n",
              "        [0.05995538, 0.12724118, 0.02547575, ..., 0.16087331,\n",
              "         0.08343313, 0.28337956],\n",
              "        [0.05995538, 0.12724118, 0.02547575, ..., 0.12640046,\n",
              "         0.08183633, 0.27738211]],\n",
              "\n",
              "       [[0.06553263, 0.13880856, 0.02547575, ..., 0.12640046,\n",
              "         0.08662675, 0.20241397],\n",
              "        [0.06553263, 0.13880856, 0.02547575, ..., 0.15512784,\n",
              "         0.08183633, 0.25039358],\n",
              "        [0.062744  , 0.15905147, 0.02547575, ..., 0.15081873,\n",
              "         0.07984032, 0.24739486],\n",
              "        ...,\n",
              "        [0.05995538, 0.12724118, 0.02547575, ..., 0.16087331,\n",
              "         0.08343313, 0.28337956],\n",
              "        [0.05995538, 0.12724118, 0.02547575, ..., 0.12640046,\n",
              "         0.08183633, 0.27738211],\n",
              "        [0.05577245, 0.14459225, 0.02547575, ..., 0.13645504,\n",
              "         0.08782435, 0.30174676]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.65923034, 0.62984384, 0.6752609 , ..., 0.72766446,\n",
              "         0.16343313, 0.06912062],\n",
              "        [0.65923034, 0.62984384, 0.6752609 , ..., 0.72134444,\n",
              "         0.16702595, 0.07076992],\n",
              "        [0.65923034, 0.62984384, 0.68201351, ..., 0.72191899,\n",
              "         0.16670659, 0.06747132],\n",
              "        ...,\n",
              "        [0.64500837, 0.6437247 , 0.67771639, ..., 0.7075553 ,\n",
              "         0.16071856, 0.06934553],\n",
              "        [0.65923034, 0.64256796, 0.68201351, ..., 0.69893709,\n",
              "         0.1607984 , 0.06994527],\n",
              "        [0.69018405, 0.65066512, 0.71608349, ..., 0.69319161,\n",
              "         0.16566866, 0.07219432]],\n",
              "\n",
              "       [[0.65923034, 0.62984384, 0.6752609 , ..., 0.72134444,\n",
              "         0.16702595, 0.07076992],\n",
              "        [0.65923034, 0.62984384, 0.68201351, ..., 0.72191899,\n",
              "         0.16670659, 0.06747132],\n",
              "        [0.65923034, 0.64256796, 0.68201351, ..., 0.71186441,\n",
              "         0.16518962, 0.06664668],\n",
              "        ...,\n",
              "        [0.65923034, 0.64256796, 0.68201351, ..., 0.69893709,\n",
              "         0.1607984 , 0.06994527],\n",
              "        [0.69018405, 0.65066512, 0.71608349, ..., 0.69319161,\n",
              "         0.16566866, 0.07219432],\n",
              "        [0.65923034, 0.64256796, 0.68201351, ..., 0.73082448,\n",
              "         0.17229541, 0.09580928]],\n",
              "\n",
              "       [[0.65923034, 0.62984384, 0.68201351, ..., 0.72191899,\n",
              "         0.16670659, 0.06747132],\n",
              "        [0.65923034, 0.64256796, 0.68201351, ..., 0.71186441,\n",
              "         0.16518962, 0.06664668],\n",
              "        [0.64500837, 0.6437247 , 0.67771639, ..., 0.7075553 ,\n",
              "         0.16071856, 0.06934553],\n",
              "        ...,\n",
              "        [0.69018405, 0.65066512, 0.71608349, ..., 0.69319161,\n",
              "         0.16566866, 0.07219432],\n",
              "        [0.65923034, 0.64256796, 0.68201351, ..., 0.73082448,\n",
              "         0.17229541, 0.09580928],\n",
              "        [0.66229782, 0.64256796, 0.68201351, ..., 0.74059178,\n",
              "         0.17053892, 0.11327686]]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lqfpnKrdG3mZ"
      },
      "outputs": [],
      "source": [
        "# Reshape input to be [samples, time steps, features]\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omUUdym3G3a-",
        "outputId": "240ae506-e6a8-4c69-fca2-a61143b7f1ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2508/2508 - 14s - 5ms/step - loss: 0.0041\n",
            "Epoch 2/100\n",
            "2508/2508 - 10s - 4ms/step - loss: 0.0015\n",
            "Epoch 3/100\n",
            "2508/2508 - 9s - 4ms/step - loss: 0.0012\n",
            "Epoch 4/100\n",
            "2508/2508 - 9s - 4ms/step - loss: 0.0011\n",
            "Epoch 5/100\n",
            "2508/2508 - 10s - 4ms/step - loss: 0.0010\n",
            "Epoch 6/100\n",
            "2508/2508 - 10s - 4ms/step - loss: 9.5507e-04\n",
            "Epoch 7/100\n",
            "2508/2508 - 9s - 4ms/step - loss: 8.9342e-04\n",
            "Epoch 8/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 8.5502e-04\n",
            "Epoch 9/100\n",
            "2508/2508 - 12s - 5ms/step - loss: 8.0431e-04\n",
            "Epoch 10/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 7.8927e-04\n",
            "Epoch 11/100\n",
            "2508/2508 - 11s - 5ms/step - loss: 7.5961e-04\n",
            "Epoch 12/100\n",
            "2508/2508 - 11s - 5ms/step - loss: 7.2823e-04\n",
            "Epoch 13/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 6.8866e-04\n",
            "Epoch 14/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 6.8332e-04\n",
            "Epoch 15/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 6.5109e-04\n",
            "Epoch 16/100\n",
            "2508/2508 - 10s - 4ms/step - loss: 6.4654e-04\n",
            "Epoch 17/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 6.3642e-04\n",
            "Epoch 18/100\n",
            "2508/2508 - 12s - 5ms/step - loss: 6.2231e-04\n",
            "Epoch 19/100\n",
            "2508/2508 - 12s - 5ms/step - loss: 5.9534e-04\n",
            "Epoch 20/100\n",
            "2508/2508 - 12s - 5ms/step - loss: 5.8456e-04\n",
            "Epoch 21/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.9326e-04\n",
            "Epoch 22/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.7633e-04\n",
            "Epoch 23/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.6766e-04\n",
            "Epoch 24/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.6304e-04\n",
            "Epoch 25/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.5888e-04\n",
            "Epoch 26/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.4524e-04\n",
            "Epoch 27/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.4033e-04\n",
            "Epoch 28/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.3474e-04\n",
            "Epoch 29/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.2933e-04\n",
            "Epoch 30/100\n",
            "2508/2508 - 11s - 5ms/step - loss: 5.2761e-04\n",
            "Epoch 31/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.1615e-04\n",
            "Epoch 32/100\n",
            "2508/2508 - 10s - 4ms/step - loss: 5.0939e-04\n",
            "Epoch 33/100\n",
            "2508/2508 - 12s - 5ms/step - loss: 5.0616e-04\n",
            "Epoch 34/100\n",
            "2508/2508 - 11s - 5ms/step - loss: 5.0188e-04\n",
            "Epoch 35/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 5.0247e-04\n",
            "Epoch 36/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.9068e-04\n",
            "Epoch 37/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.9805e-04\n",
            "Epoch 38/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.8695e-04\n",
            "Epoch 39/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.8048e-04\n",
            "Epoch 40/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.7312e-04\n",
            "Epoch 41/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.7233e-04\n",
            "Epoch 42/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.6955e-04\n",
            "Epoch 43/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.6555e-04\n",
            "Epoch 44/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.6793e-04\n",
            "Epoch 45/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.5857e-04\n",
            "Epoch 46/100\n",
            "2508/2508 - 11s - 5ms/step - loss: 4.5393e-04\n",
            "Epoch 47/100\n",
            "2508/2508 - 11s - 5ms/step - loss: 4.5351e-04\n",
            "Epoch 48/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.4957e-04\n",
            "Epoch 49/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.4067e-04\n",
            "Epoch 50/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.4505e-04\n",
            "Epoch 51/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.3737e-04\n",
            "Epoch 52/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.4164e-04\n",
            "Epoch 53/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.3081e-04\n",
            "Epoch 54/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.3182e-04\n",
            "Epoch 55/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.2249e-04\n",
            "Epoch 56/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.2983e-04\n",
            "Epoch 57/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.1863e-04\n",
            "Epoch 58/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.1850e-04\n",
            "Epoch 59/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.1615e-04\n",
            "Epoch 60/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.1007e-04\n",
            "Epoch 61/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.0913e-04\n",
            "Epoch 62/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.0954e-04\n",
            "Epoch 63/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.0480e-04\n",
            "Epoch 64/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.0072e-04\n",
            "Epoch 65/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 4.0013e-04\n",
            "Epoch 66/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.9877e-04\n",
            "Epoch 67/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.9488e-04\n",
            "Epoch 68/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.9249e-04\n",
            "Epoch 69/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.8683e-04\n",
            "Epoch 70/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.8522e-04\n",
            "Epoch 71/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.8474e-04\n",
            "Epoch 72/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.8403e-04\n",
            "Epoch 73/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.7555e-04\n",
            "Epoch 74/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.8411e-04\n",
            "Epoch 75/100\n",
            "2508/2508 - 11s - 5ms/step - loss: 3.6999e-04\n",
            "Epoch 76/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.7957e-04\n",
            "Epoch 77/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.7049e-04\n",
            "Epoch 78/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.7949e-04\n",
            "Epoch 79/100\n",
            "2508/2508 - 9s - 4ms/step - loss: 3.6817e-04\n",
            "Epoch 80/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.5962e-04\n",
            "Epoch 81/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.6791e-04\n",
            "Epoch 82/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.6490e-04\n",
            "Epoch 83/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.5987e-04\n",
            "Epoch 84/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.5664e-04\n",
            "Epoch 85/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.5865e-04\n",
            "Epoch 86/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.5750e-04\n",
            "Epoch 87/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.4884e-04\n",
            "Epoch 88/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.4517e-04\n",
            "Epoch 89/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.4610e-04\n",
            "Epoch 90/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.4069e-04\n",
            "Epoch 91/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.4746e-04\n",
            "Epoch 92/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.4081e-04\n",
            "Epoch 93/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.3901e-04\n",
            "Epoch 94/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.3551e-04\n",
            "Epoch 95/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.3822e-04\n",
            "Epoch 96/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.3609e-04\n",
            "Epoch 97/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.2755e-04\n",
            "Epoch 98/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.2934e-04\n",
            "Epoch 99/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.2903e-04\n",
            "Epoch 100/100\n",
            "2508/2508 - 11s - 4ms/step - loss: 3.2953e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x21eccee7dd0>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(X_train.shape[2]))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model\n",
        "model.save('model.keras')\n",
        "# load the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "look_back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "01mJ3I_OH_J1"
      },
      "outputs": [],
      "source": [
        "# Scale the data to be between 0 and 1\n",
        "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "last_week_data = df[crops].iloc[-7:].values \n",
        "scaled_last_week = scaler.transform(last_week_data) \n",
        "reshaped_last_week = np.reshape(scaled_last_week, (1, look_back, len(crops))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtcuDTiIH_CS",
        "outputId": "5b216073-93af-43ce-ec81-88f3860e1dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 554ms/step\n"
          ]
        }
      ],
      "source": [
        "# Predict\n",
        "prediction_scaled = model.predict(reshaped_last_week)\n",
        "prediction = scaler.inverse_transform(prediction_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-oaFh2_lH-5V"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame for 6/9/2024 predictions\n",
        "predictions_6_9_2024 = pd.DataFrame(data=prediction, columns=crops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_6_9_2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox9rmit_IH4q",
        "outputId": "b9c9d3eb-68c6-4ef1-c17e-5e4ab298798f"
      },
      "outputs": [],
      "source": [
        "# Predict for all states using the last known data\n",
        "for state, state_data in last_known_data.items():\n",
        "    scaled_state_data = scaler.transform(state_data.reshape(1,-1))\n",
        "    reshaped_state_data = np.reshape(scaled_state_data, (1, 1, len(crops)))\n",
        "\n",
        "    state_prediction_scaled = model.predict(reshaped_state_data)\n",
        "    state_prediction = scaler.inverse_transform(state_prediction_scaled)\n",
        "\n",
        "    predictions_6_9_2024.loc[state] = state_prediction[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3lNdgxFIHE-",
        "outputId": "d44ada5e-e011-4b03-82f2-e7f39c93e1b0"
      },
      "outputs": [],
      "source": [
        "# Display the predictions\n",
        "print(predictions_6_9_2024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict for all states using a rolling window of past data\n",
        "predictions_by_state = {}  # Dictionary to store predictions for each state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for state in df['States/UTs'].unique():\n",
        "    state_data = df[df['States/UTs'] == state][crops].iloc[-look_back:].values # Last 'look_back' days of data\n",
        "    scaled_state_data = scaler.transform(state_data)\n",
        "    reshaped_state_data = np.reshape(scaled_state_data, (1, look_back, len(crops)))\n",
        "\n",
        "    state_prediction_scaled = model.predict(reshaped_state_data)\n",
        "    state_prediction = scaler.inverse_transform(state_prediction_scaled)\n",
        "    predictions_by_state[state] = state_prediction[0] # Store prediction for the state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame(predictions_by_state).T # Convert to DataFrame for better display\n",
        "predictions_df.columns = crops # Add column names\n",
        "print(predictions_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test the model\n",
        "# Load the model\n",
        "model = tensorflow.keras.models.load_model('model.keras')\n",
        "\n",
        "# Load the scaler\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# it hase one state value\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
